{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 13:17:17.432769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import urllib\n",
    "import itertools\n",
    "import random, os, glob\n",
    "from imutils import paths\n",
    "from sklearn.utils import shuffle\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import  ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout, SpatialDropout2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "waste_labels = {\"cardboard\":0, \"glass\":1, \"metal\":2, \"paper\":3, \"plastic\":4, \"trash\":5}\n",
    "dir_path = \"./GarbageClassification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "  x = []\n",
    "  labels = []\n",
    "  image_paths = sorted(list(paths.list_images(path)))\n",
    "  for image_path in image_paths:\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    x.append(img)\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "    labels.append(waste_labels[label])\n",
    "  x, labels = shuffle(x, labels, random_state=42)\n",
    "  input_shape = (np.array(x[0]).shape[1], np.array(x[0]).shape[1], 3)\n",
    "  print(\"X shape: \", np.array(x).shape)\n",
    "  print(f\"Number of Labels: {len(np.unique(labels))} , Number of Observation: {len(labels)}\")\n",
    "  print(\"Input Shape: \", input_shape)\n",
    "  return x, labels, input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (2527, 224, 224, 3)\n",
      "Number of Labels: 6 , Number of Observation: 2527\n",
      "Input Shape:  (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "x, labels, input_shape = load_dataset(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization_img(image_batch, labels, num_of_img):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(num_of_img):\n",
    "    ax = plt.subplot(5,5,n+1)\n",
    "    plt.imshow(image_batch[n])\n",
    "    plt.title(np.array(list(waste_labels.keys()))[to_categorical(labels, num_classes=len(np.unique(labels)))[n] == 1][0].title())\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_img(x, labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2275 images belonging to 1 classes.\n",
      "Found 252 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16,(3,3),activation = \"relu\" , input_shape = (224, 224, 3)) ,\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32,(3,3),activation = \"relu\") ,  \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\") ,  \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128,(3,3),activation = \"relu\"),  \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(550,activation=\"relu\"),      #Adding the Hidden layer\n",
    "    tf.keras.layers.Dropout(0.1,seed = 2019),\n",
    "    tf.keras.layers.Dense(400,activation =\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3,seed = 2019),\n",
    "    tf.keras.layers.Dense(300,activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.4,seed = 2019),\n",
    "    tf.keras.layers.Dense(200,activation =\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2,seed = 2019),\n",
    "    tf.keras.layers.Dense(6,activation = \"softmax\")   #Adding the Output Layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 222, 222, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 111, 111, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 109, 109, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 550)               10138150  \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 550)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 400)               220400    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 300)               120300    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 6)                 1206      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,637,696\n",
      "Trainable params: 10,637,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,SGD,Adam\n",
    "adam=Adam(lr=0.001)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2279 images belonging to 6 classes.\n",
      "Found 250 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "bs=30         #Setting batch size\n",
    "train_dir = \"./GarbageClassification/Data/Train\"   #Setting training directory\n",
    "validation_dir = \"./GarbageClassification/Data/Valid\"   #Setting testing directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "# All images will be rescaled by 1./255.\n",
    "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "#Flow_from_directory function lets the classifier directly identify the labels from the name of the directories the image lies in\n",
    "train_generator=train_datagen.flow_from_directory(train_dir,batch_size=bs,class_mode='categorical',target_size=(224,224))\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=bs,\n",
    "                                                         class_mode  = 'categorical',\n",
    "                                                         target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 6s - loss: 1.3225 - acc: 0.4600 - val_loss: 1.2927 - val_acc: 0.4667 - 6s/epoch - 1s/step\n",
      "Epoch 2/100\n",
      "5/5 - 6s - loss: 1.3478 - acc: 0.4867 - val_loss: 1.2567 - val_acc: 0.4333 - 6s/epoch - 1s/step\n",
      "Epoch 3/100\n",
      "5/5 - 6s - loss: 1.3936 - acc: 0.4161 - val_loss: 1.3429 - val_acc: 0.5333 - 6s/epoch - 1s/step\n",
      "Epoch 4/100\n",
      "5/5 - 6s - loss: 1.2795 - acc: 0.4933 - val_loss: 1.2528 - val_acc: 0.4667 - 6s/epoch - 1s/step\n",
      "Epoch 5/100\n",
      "5/5 - 6s - loss: 1.2897 - acc: 0.4667 - val_loss: 1.5128 - val_acc: 0.3333 - 6s/epoch - 1s/step\n",
      "Epoch 6/100\n",
      "5/5 - 6s - loss: 1.3286 - acc: 0.4933 - val_loss: 1.1825 - val_acc: 0.4333 - 6s/epoch - 1s/step\n",
      "Epoch 7/100\n",
      "5/5 - 9s - loss: 1.3321 - acc: 0.4867 - val_loss: 1.4587 - val_acc: 0.4000 - 9s/epoch - 2s/step\n",
      "Epoch 8/100\n",
      "5/5 - 8s - loss: 1.2052 - acc: 0.5200 - val_loss: 1.2881 - val_acc: 0.3000 - 8s/epoch - 2s/step\n",
      "Epoch 9/100\n",
      "5/5 - 9s - loss: 1.4153 - acc: 0.4333 - val_loss: 1.5266 - val_acc: 0.3000 - 9s/epoch - 2s/step\n",
      "Epoch 10/100\n",
      "5/5 - 7s - loss: 1.3237 - acc: 0.5067 - val_loss: 1.6663 - val_acc: 0.4667 - 7s/epoch - 1s/step\n",
      "Epoch 11/100\n",
      "5/5 - 7s - loss: 1.3711 - acc: 0.4467 - val_loss: 1.1603 - val_acc: 0.5333 - 7s/epoch - 1s/step\n",
      "Epoch 12/100\n",
      "5/5 - 8s - loss: 1.4035 - acc: 0.4200 - val_loss: 1.4532 - val_acc: 0.3667 - 8s/epoch - 2s/step\n",
      "Epoch 13/100\n",
      "5/5 - 8s - loss: 1.2287 - acc: 0.5200 - val_loss: 1.0239 - val_acc: 0.5667 - 8s/epoch - 2s/step\n",
      "Epoch 14/100\n",
      "5/5 - 7s - loss: 1.2106 - acc: 0.5436 - val_loss: 1.0180 - val_acc: 0.5667 - 7s/epoch - 1s/step\n",
      "Epoch 15/100\n",
      "5/5 - 7s - loss: 1.3151 - acc: 0.5000 - val_loss: 1.3210 - val_acc: 0.5667 - 7s/epoch - 1s/step\n",
      "Epoch 16/100\n",
      "5/5 - 6s - loss: 1.3516 - acc: 0.4733 - val_loss: 1.0859 - val_acc: 0.6000 - 6s/epoch - 1s/step\n",
      "Epoch 17/100\n",
      "5/5 - 6s - loss: 1.2897 - acc: 0.5400 - val_loss: 1.3943 - val_acc: 0.4000 - 6s/epoch - 1s/step\n",
      "Epoch 18/100\n",
      "5/5 - 7s - loss: 1.2617 - acc: 0.5267 - val_loss: 0.9696 - val_acc: 0.6000 - 7s/epoch - 1s/step\n",
      "Epoch 19/100\n",
      "5/5 - 6s - loss: 1.1965 - acc: 0.5333 - val_loss: 1.0823 - val_acc: 0.4667 - 6s/epoch - 1s/step\n",
      "Epoch 20/100\n",
      "5/5 - 7s - loss: 1.2709 - acc: 0.5200 - val_loss: 1.7518 - val_acc: 0.4333 - 7s/epoch - 1s/step\n",
      "Epoch 21/100\n",
      "5/5 - 8s - loss: 1.3602 - acc: 0.5667 - val_loss: 1.6482 - val_acc: 0.3000 - 8s/epoch - 2s/step\n",
      "Epoch 22/100\n",
      "5/5 - 8s - loss: 1.7030 - acc: 0.3600 - val_loss: 1.5933 - val_acc: 0.3333 - 8s/epoch - 2s/step\n",
      "Epoch 23/100\n",
      "5/5 - 7s - loss: 1.4824 - acc: 0.4333 - val_loss: 1.6201 - val_acc: 0.3333 - 7s/epoch - 1s/step\n",
      "Epoch 24/100\n",
      "5/5 - 8s - loss: 1.5591 - acc: 0.3867 - val_loss: 1.3759 - val_acc: 0.4667 - 8s/epoch - 2s/step\n",
      "Epoch 25/100\n",
      "5/5 - 7s - loss: 1.4579 - acc: 0.4333 - val_loss: 1.2902 - val_acc: 0.5000 - 7s/epoch - 1s/step\n",
      "Epoch 26/100\n",
      "5/5 - 7s - loss: 1.3098 - acc: 0.5168 - val_loss: 1.0331 - val_acc: 0.6333 - 7s/epoch - 1s/step\n",
      "Epoch 27/100\n",
      "5/5 - 6s - loss: 1.2891 - acc: 0.4667 - val_loss: 1.0794 - val_acc: 0.5667 - 6s/epoch - 1s/step\n",
      "Epoch 28/100\n",
      "5/5 - 6s - loss: 1.0683 - acc: 0.6200 - val_loss: 1.3044 - val_acc: 0.4667 - 6s/epoch - 1s/step\n",
      "Epoch 29/100\n",
      "5/5 - 6s - loss: 1.1709 - acc: 0.5333 - val_loss: 1.1775 - val_acc: 0.5333 - 6s/epoch - 1s/step\n",
      "Epoch 30/100\n",
      "5/5 - 6s - loss: 1.2020 - acc: 0.5000 - val_loss: 1.3004 - val_acc: 0.5000 - 6s/epoch - 1s/step\n",
      "Epoch 31/100\n",
      "5/5 - 6s - loss: 1.1506 - acc: 0.5667 - val_loss: 1.0537 - val_acc: 0.5333 - 6s/epoch - 1s/step\n",
      "Epoch 32/100\n",
      "5/5 - 6s - loss: 1.2253 - acc: 0.4600 - val_loss: 1.0564 - val_acc: 0.6000 - 6s/epoch - 1s/step\n",
      "Epoch 33/100\n",
      "5/5 - 6s - loss: 1.1034 - acc: 0.5867 - val_loss: 1.1109 - val_acc: 0.4667 - 6s/epoch - 1s/step\n",
      "Epoch 34/100\n",
      "5/5 - 6s - loss: 1.0841 - acc: 0.5133 - val_loss: 1.1657 - val_acc: 0.5667 - 6s/epoch - 1s/step\n",
      "Epoch 35/100\n",
      "5/5 - 5s - loss: 0.9985 - acc: 0.6067 - val_loss: 1.0999 - val_acc: 0.5667 - 5s/epoch - 1s/step\n",
      "Epoch 36/100\n",
      "5/5 - 6s - loss: 1.0997 - acc: 0.5600 - val_loss: 1.3124 - val_acc: 0.4667 - 6s/epoch - 1s/step\n",
      "Epoch 37/100\n",
      "5/5 - 7s - loss: 1.0769 - acc: 0.5533 - val_loss: 1.2777 - val_acc: 0.4667 - 7s/epoch - 1s/step\n",
      "Epoch 38/100\n",
      "5/5 - 8s - loss: 1.0060 - acc: 0.5933 - val_loss: 1.3391 - val_acc: 0.5333 - 8s/epoch - 2s/step\n",
      "Epoch 39/100\n",
      "5/5 - 7s - loss: 1.0671 - acc: 0.6000 - val_loss: 0.8776 - val_acc: 0.7000 - 7s/epoch - 1s/step\n",
      "Epoch 40/100\n",
      "5/5 - 6s - loss: 1.0313 - acc: 0.6067 - val_loss: 1.1727 - val_acc: 0.5333 - 6s/epoch - 1s/step\n",
      "Epoch 41/100\n",
      "5/5 - 7s - loss: 1.1492 - acc: 0.5867 - val_loss: 1.1338 - val_acc: 0.5333 - 7s/epoch - 1s/step\n",
      "Epoch 42/100\n",
      "5/5 - 7s - loss: 0.9992 - acc: 0.6267 - val_loss: 1.0983 - val_acc: 0.6667 - 7s/epoch - 1s/step\n",
      "Epoch 43/100\n",
      "5/5 - 7s - loss: 0.8788 - acc: 0.6600 - val_loss: 1.2527 - val_acc: 0.5000 - 7s/epoch - 1s/step\n",
      "Epoch 44/100\n",
      "5/5 - 7s - loss: 1.0818 - acc: 0.6400 - val_loss: 1.1912 - val_acc: 0.5333 - 7s/epoch - 1s/step\n",
      "Epoch 45/100\n",
      "5/5 - 7s - loss: 0.9505 - acc: 0.6200 - val_loss: 1.0518 - val_acc: 0.6667 - 7s/epoch - 1s/step\n",
      "Epoch 46/100\n",
      "5/5 - 7s - loss: 1.0685 - acc: 0.5600 - val_loss: 1.0962 - val_acc: 0.5333 - 7s/epoch - 1s/step\n",
      "Epoch 47/100\n",
      "5/5 - 6s - loss: 1.0967 - acc: 0.6133 - val_loss: 1.1286 - val_acc: 0.5333 - 6s/epoch - 1s/step\n",
      "Epoch 48/100\n",
      "5/5 - 6s - loss: 1.0652 - acc: 0.6133 - val_loss: 1.2132 - val_acc: 0.3667 - 6s/epoch - 1s/step\n",
      "Epoch 49/100\n",
      "5/5 - 6s - loss: 0.9007 - acc: 0.7267 - val_loss: 1.2316 - val_acc: 0.4333 - 6s/epoch - 1s/step\n",
      "Epoch 50/100\n",
      "5/5 - 7s - loss: 1.1127 - acc: 0.5867 - val_loss: 1.1620 - val_acc: 0.5667 - 7s/epoch - 1s/step\n",
      "Epoch 51/100\n",
      "5/5 - 6s - loss: 1.1360 - acc: 0.5600 - val_loss: 1.1018 - val_acc: 0.5000 - 6s/epoch - 1s/step\n",
      "Epoch 52/100\n",
      "5/5 - 5s - loss: 1.0673 - acc: 0.6040 - val_loss: 1.1015 - val_acc: 0.6667 - 5s/epoch - 1s/step\n",
      "Epoch 53/100\n",
      "5/5 - 6s - loss: 1.1155 - acc: 0.6200 - val_loss: 1.0793 - val_acc: 0.5000 - 6s/epoch - 1s/step\n",
      "Epoch 54/100\n",
      "5/5 - 6s - loss: 0.9062 - acc: 0.6867 - val_loss: 0.9419 - val_acc: 0.5667 - 6s/epoch - 1s/step\n",
      "Epoch 55/100\n",
      "5/5 - 6s - loss: 0.9093 - acc: 0.6667 - val_loss: 1.0483 - val_acc: 0.6000 - 6s/epoch - 1s/step\n",
      "Epoch 56/100\n",
      "5/5 - 6s - loss: 0.9059 - acc: 0.6667 - val_loss: 0.8713 - val_acc: 0.6667 - 6s/epoch - 1s/step\n",
      "Epoch 57/100\n",
      "5/5 - 6s - loss: 0.8357 - acc: 0.6800 - val_loss: 1.2315 - val_acc: 0.4667 - 6s/epoch - 1s/step\n",
      "Epoch 58/100\n",
      "5/5 - 7s - loss: 0.8182 - acc: 0.6867 - val_loss: 1.1465 - val_acc: 0.5333 - 7s/epoch - 1s/step\n",
      "Epoch 59/100\n",
      "5/5 - 10s - loss: 0.8680 - acc: 0.7000 - val_loss: 1.5078 - val_acc: 0.5667 - 10s/epoch - 2s/step\n",
      "Epoch 60/100\n",
      "5/5 - 9s - loss: 1.0115 - acc: 0.6667 - val_loss: 1.6422 - val_acc: 0.5000 - 9s/epoch - 2s/step\n",
      "Epoch 61/100\n",
      "5/5 - 9s - loss: 0.8934 - acc: 0.6867 - val_loss: 1.1462 - val_acc: 0.4667 - 9s/epoch - 2s/step\n",
      "Epoch 62/100\n",
      "5/5 - 9s - loss: 1.0420 - acc: 0.6733 - val_loss: 1.0553 - val_acc: 0.6333 - 9s/epoch - 2s/step\n",
      "Epoch 63/100\n",
      "5/5 - 9s - loss: 0.8352 - acc: 0.7200 - val_loss: 1.0591 - val_acc: 0.5667 - 9s/epoch - 2s/step\n",
      "Epoch 64/100\n",
      "5/5 - 9s - loss: 0.9528 - acc: 0.6400 - val_loss: 1.3532 - val_acc: 0.4667 - 9s/epoch - 2s/step\n",
      "Epoch 65/100\n",
      "5/5 - 11s - loss: 0.8753 - acc: 0.7067 - val_loss: 0.9531 - val_acc: 0.6667 - 11s/epoch - 2s/step\n",
      "Epoch 66/100\n",
      "5/5 - 7s - loss: 0.8117 - acc: 0.7000 - val_loss: 1.1737 - val_acc: 0.5667 - 7s/epoch - 1s/step\n",
      "Epoch 67/100\n",
      "5/5 - 7s - loss: 0.8524 - acc: 0.7467 - val_loss: 0.9799 - val_acc: 0.6667 - 7s/epoch - 1s/step\n",
      "Epoch 68/100\n",
      "5/5 - 7s - loss: 0.9418 - acc: 0.6867 - val_loss: 1.0936 - val_acc: 0.7000 - 7s/epoch - 1s/step\n",
      "Epoch 69/100\n",
      "5/5 - 6s - loss: 0.7495 - acc: 0.7333 - val_loss: 1.0702 - val_acc: 0.6333 - 6s/epoch - 1s/step\n",
      "Epoch 70/100\n",
      "5/5 - 6s - loss: 0.9845 - acc: 0.6400 - val_loss: 1.0412 - val_acc: 0.7000 - 6s/epoch - 1s/step\n",
      "Epoch 71/100\n",
      "5/5 - 8s - loss: 0.7977 - acc: 0.7133 - val_loss: 1.0153 - val_acc: 0.6000 - 8s/epoch - 2s/step\n",
      "Epoch 72/100\n",
      "5/5 - 8s - loss: 0.6353 - acc: 0.7733 - val_loss: 0.9582 - val_acc: 0.5667 - 8s/epoch - 2s/step\n",
      "Epoch 73/100\n",
      "5/5 - 7s - loss: 0.6794 - acc: 0.7400 - val_loss: 0.9599 - val_acc: 0.6667 - 7s/epoch - 1s/step\n",
      "Epoch 74/100\n",
      "5/5 - 7s - loss: 0.6626 - acc: 0.7667 - val_loss: 1.4999 - val_acc: 0.5000 - 7s/epoch - 1s/step\n",
      "Epoch 75/100\n",
      "5/5 - 7s - loss: 0.5974 - acc: 0.7800 - val_loss: 1.1204 - val_acc: 0.5000 - 7s/epoch - 1s/step\n",
      "Epoch 76/100\n",
      "5/5 - 7s - loss: 0.6595 - acc: 0.8067 - val_loss: 1.2290 - val_acc: 0.6333 - 7s/epoch - 1s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "5/5 - 6s - loss: 0.5899 - acc: 0.7800 - val_loss: 1.1377 - val_acc: 0.6000 - 6s/epoch - 1s/step\n",
      "Epoch 78/100\n",
      "5/5 - 7s - loss: 0.6497 - acc: 0.7987 - val_loss: 0.8593 - val_acc: 0.7000 - 7s/epoch - 1s/step\n",
      "Epoch 79/100\n",
      "5/5 - 7s - loss: 0.6877 - acc: 0.8200 - val_loss: 1.2976 - val_acc: 0.6333 - 7s/epoch - 1s/step\n",
      "Epoch 80/100\n",
      "5/5 - 9s - loss: 0.8196 - acc: 0.7267 - val_loss: 1.0563 - val_acc: 0.7333 - 9s/epoch - 2s/step\n",
      "Epoch 81/100\n",
      "5/5 - 7s - loss: 0.7474 - acc: 0.7400 - val_loss: 1.0967 - val_acc: 0.5333 - 7s/epoch - 1s/step\n",
      "Epoch 82/100\n",
      "5/5 - 9s - loss: 0.7485 - acc: 0.7800 - val_loss: 0.8072 - val_acc: 0.7667 - 9s/epoch - 2s/step\n",
      "Epoch 83/100\n",
      "5/5 - 7s - loss: 0.6672 - acc: 0.7533 - val_loss: 0.8015 - val_acc: 0.6667 - 7s/epoch - 1s/step\n",
      "Epoch 84/100\n",
      "5/5 - 6s - loss: 0.6506 - acc: 0.7533 - val_loss: 0.8696 - val_acc: 0.7333 - 6s/epoch - 1s/step\n",
      "Epoch 85/100\n",
      "5/5 - 7s - loss: 0.5071 - acc: 0.8267 - val_loss: 1.1735 - val_acc: 0.6000 - 7s/epoch - 1s/step\n",
      "Epoch 86/100\n",
      "5/5 - 6s - loss: 0.6532 - acc: 0.7867 - val_loss: 0.7588 - val_acc: 0.7000 - 6s/epoch - 1s/step\n",
      "Epoch 87/100\n",
      "5/5 - 6s - loss: 0.5973 - acc: 0.8200 - val_loss: 1.1177 - val_acc: 0.7333 - 6s/epoch - 1s/step\n",
      "Epoch 88/100\n",
      "5/5 - 7s - loss: 0.5662 - acc: 0.8333 - val_loss: 0.7949 - val_acc: 0.7000 - 7s/epoch - 1s/step\n",
      "Epoch 89/100\n",
      "5/5 - 9s - loss: 0.5089 - acc: 0.8200 - val_loss: 1.0449 - val_acc: 0.6333 - 9s/epoch - 2s/step\n",
      "Epoch 90/100\n",
      "5/5 - 9s - loss: 0.5232 - acc: 0.8267 - val_loss: 0.7368 - val_acc: 0.8000 - 9s/epoch - 2s/step\n",
      "Epoch 91/100\n",
      "5/5 - 9s - loss: 0.5080 - acc: 0.8067 - val_loss: 1.3046 - val_acc: 0.4667 - 9s/epoch - 2s/step\n",
      "Epoch 92/100\n",
      "5/5 - 7s - loss: 0.7032 - acc: 0.7400 - val_loss: 1.1582 - val_acc: 0.7333 - 7s/epoch - 1s/step\n",
      "Epoch 93/100\n",
      "5/5 - 7s - loss: 0.7730 - acc: 0.7733 - val_loss: 1.1685 - val_acc: 0.6000 - 7s/epoch - 1s/step\n",
      "Epoch 94/100\n",
      "5/5 - 10s - loss: 0.5441 - acc: 0.8467 - val_loss: 1.1336 - val_acc: 0.5667 - 10s/epoch - 2s/step\n",
      "Epoch 95/100\n",
      "5/5 - 9s - loss: 0.6135 - acc: 0.8200 - val_loss: 0.6142 - val_acc: 0.8333 - 9s/epoch - 2s/step\n",
      "Epoch 96/100\n",
      "5/5 - 9s - loss: 0.4561 - acc: 0.8533 - val_loss: 0.7134 - val_acc: 0.7333 - 9s/epoch - 2s/step\n",
      "Epoch 97/100\n",
      "5/5 - 7s - loss: 0.4704 - acc: 0.8600 - val_loss: 1.0177 - val_acc: 0.6000 - 7s/epoch - 1s/step\n",
      "Epoch 98/100\n",
      "5/5 - 8s - loss: 0.4502 - acc: 0.8333 - val_loss: 0.9519 - val_acc: 0.6667 - 8s/epoch - 2s/step\n",
      "Epoch 99/100\n",
      "5/5 - 7s - loss: 0.2753 - acc: 0.9200 - val_loss: 1.0868 - val_acc: 0.7667 - 7s/epoch - 1s/step\n",
      "Epoch 100/100\n",
      "5/5 - 7s - loss: 0.3123 - acc: 0.8933 - val_loss: 1.2092 - val_acc: 0.6000 - 7s/epoch - 1s/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    steps_per_epoch=150 // bs,\n",
    "                    epochs=100,\n",
    "                    validation_steps=50 // bs,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model_evaluate(model):\n",
    "  loss, precision, recall, acc = model.evaluate(test_generator, batch_size=32)\n",
    "  print(\"Test Accuracy: %.2f\" % (100 * acc))\n",
    "  print(\"Test Loss: %.2f\" % (100 * loss))\n",
    "  print(\"Test Precision: %.2f\" % (100 * precision))\n",
    "  print(\"Test Recall: %.2f\" % (100 * recall))\n",
    "\n",
    "  X_test, y_test = test_generator.next()\n",
    "  y_pred = model.predict(X_test)\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "  y_test = np.argmax(y_test, axis=1)\n",
    "  target_names = list(waste_labels.keys())\n",
    "  print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "  plt.figure(figsize=(20,5))\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.plot(history.history[\"acc\"], color=\"r\", label=\"Training Accuracy\")\n",
    "  plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation Accuracy\")\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.xlabel(\"Epoch\", fontsize=16)\n",
    "  plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "  plt.ylim([min(plt.ylim()),1])\n",
    "  plt.title(\"Training and Validation Accuracy\", fontsize=16)\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.plot(history.history[\"loss\"], color=\"r\", label=\"Training Loss\")\n",
    "  plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation Loss\")\n",
    "  plt.legend(loc=\"upper right\")\n",
    "  plt.xlabel(\"Epoch\", fontsize=16)\n",
    "  plt.ylabel(\"Loss\", fontsize=16)\n",
    "  plt.ylim([0, max(plt.ylim())])\n",
    "  plt.title(\"Training and Validation Loss\", fontsize=16)\n",
    "\n",
    "  return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./currentmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './GarbageClassification/Garbage classification/Garbage classification/cardboard/cardboard184.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_test, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_model_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mCNN_model_evaluate\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCNN_model_evaluate\u001b[39m(model):\n\u001b[0;32m----> 2\u001b[0m   loss, precision, recall, acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m acc))\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m loss))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[1;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    423\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './GarbageClassification/Garbage classification/Garbage classification/cardboard/cardboard184.jpg'"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = CNN_model_evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.OrRd):\n",
    "  if normalize:\n",
    "    cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "  \n",
    "  plt.figure(figsize=(8,6))\n",
    "  plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(classes))\n",
    "  plt.xticks(tick_marks, classes, rotation=45)\n",
    "  plt.yticks(tick_marks, classes)\n",
    "  fmt = \".2f\" if normalize else \"d\"\n",
    "  thresh = cm.max() / 2.\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel(\"True Labels\", fontweight=\"bold\")\n",
    "  plt.xlabel(\"Predicted Labels\", fontweight=\"bold\")\n",
    "\n",
    "plot_confusion_matrix(cm, waste_labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model_testing(path):\n",
    "  img = image.load_img(path, target_size=(target_size))\n",
    "  img = image.img_to_array(img, dtype=np.uint8)\n",
    "  img = np.array(img)/255.0\n",
    "  p = model.predict(img.reshape(1,224,224,3))\n",
    "  predicted_class = np.argmax(p[0])\n",
    "  return img, p, predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    }
   ],
   "source": [
    "img, p, predicted_class = CNN_model_testing(\"./glass51.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1965e-07,     0.99997,  1.1308e-05,   8.157e-08,  2.1339e-05,  4.2926e-08]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
